{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPT Text Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from pptx import Presentation\n",
    "\n",
    "\n",
    "#Function to Extract Text from PPT\n",
    "def extract_text_from_ppt(file):\n",
    "    prs = Presentation(file.name) #load\n",
    "    text_data = [] #store\n",
    "\n",
    "    for slide_number, slide in enumerate(prs.slides, start=1):\n",
    "        slide_text = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\") and shape.text.strip():\n",
    "                slide_text.append(shape.text.strip())\n",
    "\n",
    "        if slide_text:\n",
    "            text_data.append(f\"Slide {slide_number}:\\n\" + \"\\n\".join(slide_text))\n",
    "\n",
    "    return \"\\n\\n\".join(text_data) if text_data else \"No text found in the PPT.\"\n",
    "\n",
    "# Create Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=extract_text_from_ppt,\n",
    "    inputs=gr.File(label=\"Upload PPT File\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"PPT Text Extractor\",\n",
    "    description=\"Upload a .pptx file to extract text from slides.\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PRADIPTI CHAKRABORTY\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "def summarize_presentation(text):\n",
    "    summary = summarizer(text)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPT Image OCR Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from pptx import Presentation\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "import gradio as gr\n",
    "import re\n",
    "\n",
    "# Set the Tesseract OCR path\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Apply noise removal and thresholding to enhance OCR accuracy\"\"\"\n",
    "    open_cv_image = np.array(image.convert(\"L\"))  # Convert to grayscale\n",
    "\n",
    "    # Apply Gaussian Blur (Reduces noise)\n",
    "    blurred = cv2.GaussianBlur(open_cv_image, (5, 5), 0)\n",
    "\n",
    "    # Adaptive Thresholding for better contrast\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    # Apply Morphological Opening to remove small noise\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    clean_image = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    return Image.fromarray(clean_image)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Filter out noisy characters, symbols, and random text\"\"\"\n",
    "    text = text.strip()\n",
    "\n",
    "    # Remove common OCR artifacts\n",
    "    text = re.sub(r\"[|*_~‚Äî@\\[\\]]+\", \"\", text)  \n",
    "\n",
    "    # Remove lines with too many symbols (probably noise)\n",
    "    text = \"\\n\".join(line for line in text.split(\"\\n\") if sum(c.isalnum() for c in line) > len(line) * 0.5)\n",
    "\n",
    "    # Remove excess whitespace\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text if len(text) > 3 else \"\"  # Ignore extremely short garbage text\n",
    "\n",
    "def extract_text_from_images(ppt_file):\n",
    "    \"\"\"Extracts text from images in PPT slides using OCR\"\"\"\n",
    "    prs = Presentation(ppt_file)\n",
    "    extracted_text = []\n",
    "\n",
    "    for slide_number, slide in enumerate(prs.slides, start=1):\n",
    "        slide_text = f\"Slide {slide_number}:\\n\"\n",
    "\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"image\"):\n",
    "                image_stream = io.BytesIO(shape.image.blob)\n",
    "                image = Image.open(image_stream)\n",
    "                image = preprocess_image(image)  # Apply enhanced image processing\n",
    "\n",
    "                # Extract text using pytesseract with better settings\n",
    "                text = pytesseract.image_to_string(image, lang=\"eng\", config=\"--psm 6 --oem 3\")\n",
    "                text = clean_text(text)  # Clean OCR output\n",
    "                \n",
    "                if text:  # Only add meaningful text\n",
    "                    slide_text += f\"\\n[Image Text]: {text}\\n\"\n",
    "\n",
    "        extracted_text.append(slide_text)\n",
    "\n",
    "    return \"\\n\".join(extracted_text) if extracted_text else \"No readable text found in images.\"\n",
    "\n",
    "def gradio_interface(file):\n",
    "    if file is None:\n",
    "        return \"Please upload a PPT file.\"\n",
    "    \n",
    "    extracted_text = extract_text_from_images(file.name)\n",
    "    return extracted_text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface, \n",
    "    inputs=gr.File(label=\"Upload PowerPoint File (.pptx)\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"PPT Image OCR Extractor\",\n",
    "    description=\"Upload a PowerPoint file to extract text from images using OCR.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "direct summeerise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from pptx import Presentation\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the summarization model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Function to extract text from PPT\n",
    "def extract_text_from_ppt(file):\n",
    "    prs = Presentation(file.name)  # Load PPT\n",
    "    text_data = []  # Store extracted text\n",
    "\n",
    "    for slide_number, slide in enumerate(prs.slides, start=1):\n",
    "        slide_text = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\") and shape.text.strip():\n",
    "                slide_text.append(shape.text.strip())\n",
    "\n",
    "        if slide_text:\n",
    "            text_data.append(f\"Slide {slide_number}:\\n\" + \"\\n\".join(slide_text))\n",
    "\n",
    "    extracted_text = \"\\n\\n\".join(text_data) if text_data else \"No text found in the PPT.\"\n",
    "    return extracted_text\n",
    "\n",
    "# Function to chunk long text\n",
    "def chunk_text(text, max_length=1024):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(\" \".join(current_chunk)) > max_length:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "# Function to summarize extracted text\n",
    "def summarize_ppt(file):\n",
    "    extracted_text = extract_text_from_ppt(file)  # Extract text\n",
    "    if extracted_text == \"No text found in the PPT.\":\n",
    "        return extracted_text\n",
    "\n",
    "    chunks = chunk_text(extracted_text)  # Chunk text if needed\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(chunk, max_length=200, min_length=50, do_sample=False)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=summarize_ppt,\n",
    "    inputs=gr.File(label=\"Upload PPT File\"),\n",
    "    outputs=gr.Textbox(label=\"Summarized Text\"),\n",
    "    title=\"AI-Powered PPT Summarizer\",\n",
    "    description=\"Upload a .pptx file, and this tool will extract and summarize its content using AI.\",\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "from langdetect import detect\n",
    "import language_tool_python\n",
    "import speech_recognition as sr\n",
    "import re\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Initialize the recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to convert audio to WAV if it's not already in that format\n",
    "def convert_to_wav(audio_file):\n",
    "    if audio_file.lower().endswith(('.mp3', '.ogg', '.m4a', '.acc')):\n",
    "        audio = AudioSegment.from_file(audio_file)\n",
    "        wav_file = \"converted_audio.wav\"\n",
    "        audio.export(wav_file, format=\"wav\")\n",
    "        return wav_file\n",
    "    return audio_file\n",
    "\n",
    "# Function to process audio with Google STT\n",
    "def process_audio(file):\n",
    "    # Convert audio to WAV if necessary\n",
    "    file = convert_to_wav(file)\n",
    "\n",
    "    # Step 1: Use Google Speech-to-Text to transcribe\n",
    "    with sr.AudioFile(file) as source:\n",
    "        audio = recognizer.record(source)\n",
    "        \n",
    "    try:\n",
    "        transcribed_text = recognizer.recognize_google(audio, language=\"en-IN\")  # Use \"en-IN\" for multilingual support\n",
    "    except Exception as e:\n",
    "        return {\"Error\": f\"Error transcribing audio: {str(e)}\"}\n",
    "\n",
    "    # Step 2: Separate English and Hindi words\n",
    "    words = re.findall(r'\\b\\w+\\b', transcribed_text)\n",
    "    english_words = []\n",
    "    hindi_words = []\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            lang = detect(word)\n",
    "            if lang == 'en':\n",
    "                english_words.append(word)\n",
    "            elif lang == 'hi':\n",
    "                hindi_words.append(word)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Save to files\n",
    "    with open(\"english_words.txt\", \"w\", encoding=\"utf-8\") as f_en:\n",
    "        f_en.write(\" \".join(english_words))\n",
    "\n",
    "    with open(\"hindi_words.txt\", \"w\", encoding=\"utf-8\") as f_hi:\n",
    "        f_hi.write(\" \".join(hindi_words))\n",
    "\n",
    "    # Step 3: Check if English sentence is meaningful\n",
    "    english_sentence = \" \".join(english_words)\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "    matches = tool.check(english_sentence)\n",
    "    is_meaningful = len(matches) == 0\n",
    "\n",
    "    return {\n",
    "        \"Transcribed Text\": transcribed_text,\n",
    "        \"English Sentence\": english_sentence,\n",
    "        \"Is Sentence Meaningful?\": is_meaningful,\n",
    "        \"English Words File\": \"english_words.txt\",\n",
    "        \"Hindi Words File\": \"hindi_words.txt\"\n",
    "    }\n",
    "\n",
    "# Gradio UI\n",
    "interface = gr.Interface(\n",
    "    fn=process_audio,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"Upload Audio\"),\n",
    "    outputs=\"json\",\n",
    "    title=\"Multilingual Audio Processor with Google STT\"\n",
    ")\n",
    "\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import re\n",
    "from translate import Translator\n",
    "import langdetect\n",
    "\n",
    "def translate_hinglish(text, to_lang=\"en\"):\n",
    "    \"\"\"Translates text, handling Hinglish words by word.\"\"\"\n",
    "    translator = Translator(to_lang=to_lang)\n",
    "    words = text.split()\n",
    "    translated_words = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            lang = langdetect.detect(word)\n",
    "            if lang == \"en\" or lang == \"hi\":\n",
    "                translated_words.append(translator.translate(word))\n",
    "            else:\n",
    "                translated_words.append(word) # Keep Hinglish as is.\n",
    "        except langdetect.LangDetectException:\n",
    "            translated_words.append(word) # Keep unknown words as is.\n",
    "    return \" \".join(translated_words)\n",
    "\n",
    "def transcribe_and_translate_audio(audio_file_path, output_file_path=\"transcription.txt\"):\n",
    "    \"\"\"Transcribes audio, translates (handling Hinglish), and segregates words.\"\"\"\n",
    "\n",
    "    recognizer = sr.Recognizer()\n",
    "    try:\n",
    "        _, file_extension = os.path.splitext(audio_file_path)\n",
    "        file_extension = file_extension.lower()\n",
    "\n",
    "        if file_extension not in [\".wav\", \".aiff\", \".aiff-c\", \".flac\"]:\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_wav:\n",
    "                try:\n",
    "                    subprocess.run(\n",
    "                        [\"ffmpeg\", \"-i\", audio_file_path, temp_wav.name],\n",
    "                        check=True,\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                    )\n",
    "                    audio_file_path = temp_wav.name\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(f\"Error during audio conversion: {e.stderr}\")\n",
    "                    return\n",
    "\n",
    "        with sr.AudioFile(audio_file_path) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "\n",
    "        try:\n",
    "            transcription = recognizer.recognize_google(audio_data, language=\"en-IN,hi-IN\")\n",
    "        except sr.UnknownValueError:\n",
    "            transcription = \"Could not understand audio\"\n",
    "        except sr.RequestError as e:\n",
    "            transcription = f\"Could not request results from Google Speech Recognition service; {e}\"\n",
    "\n",
    "        if transcription == \"Could not understand audio\" or \"Could not request results from Google Speech Recognition service\" in transcription:\n",
    "            with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(transcription)\n",
    "            return\n",
    "\n",
    "        words = transcription.split()\n",
    "        language_words = {\"en\": [], \"hi\": [], \"hinglish\": [], \"unknown\": []}\n",
    "\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            try:\n",
    "                if re.search(r'[a-zA-Z][\\u0900-\\u097F]', word) or re.search(r'[\\u0900-\\u097F][a-zA-Z]', word) or re.search(r'[a-zA-Z]+\\d+[a-zA-Z]*', word):\n",
    "                    language_words[\"hinglish\"].append(word)\n",
    "                elif re.search(r'[\\u0900-\\u097F]+', word):\n",
    "                    language_words[\"hi\"].append(word)\n",
    "                elif re.search(r'[a-zA-Z]+', word):\n",
    "                    language_words[\"en\"].append(word)\n",
    "                else:\n",
    "                    language_words[\"unknown\"].append(word)\n",
    "            except Exception as e:\n",
    "                language_words[\"unknown\"].append(word)\n",
    "\n",
    "        try:\n",
    "            translated_text = translate_hinglish(transcription)\n",
    "        except Exception as translation_error:\n",
    "            translated_text = f\"Translation failed: {translation_error}\"\n",
    "\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(\"Raw Transcription:\\n\" + transcription + \"\\n\\n\")\n",
    "            file.write(\"Translated Transcription(English):\\n\" + translated_text + \"\\n\\n\")\n",
    "            file.write(\"English: \" + \" \".join(language_words[\"en\"]) + \"\\n\")\n",
    "            file.write(\"Hindi: \" + \" \".join(language_words[\"hi\"]) + \"\\n\")\n",
    "            file.write(\"Hinglish: \" + \" \".join(language_words[\"hinglish\"]) + \"\\n\")\n",
    "            file.write(\"Unknown: \" + \" \".join(language_words[\"unknown\"]))\n",
    "\n",
    "        print(f\"Transcription, translation, and language segregation saved to {output_file_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Audio file not found at {audio_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "audio_file = \"phase 1 ppt.wav\"\n",
    "if os.path.exists(audio_file):\n",
    "    transcribe_and_translate_audio(audio_file)\n",
    "else:\n",
    "    print(f\"Audio file '{audio_file}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pptx import Presentation\n",
    "from translate import Translator\n",
    "import langdetect\n",
    "\n",
    "def translate_hinglish(text, to_lang=\"en\"):\n",
    "    \"\"\"Translates text, handling Hinglish words by word.\"\"\"\n",
    "    translator = Translator(to_lang=to_lang)\n",
    "    words = text.split()\n",
    "    translated_words = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            lang = langdetect.detect(word)\n",
    "            if lang == \"en\" or lang == \"hi\":\n",
    "                translated_words.append(translator.translate(word))\n",
    "            else:\n",
    "                translated_words.append(word)  # Keep Hinglish as is.\n",
    "        except langdetect.LangDetectException:\n",
    "            translated_words.append(word)  # Keep unknown words as is.\n",
    "    return \" \".join(translated_words)\n",
    "\n",
    "def extract_text_from_ppt(ppt_file_path, output_file_path=\"extracted_text.txt\"):\n",
    "    \"\"\"Extracts text from PPT slides, translates (handling Hinglish), and saves to a file.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Check if the PPT file exists\n",
    "        if not os.path.exists(ppt_file_path):\n",
    "            print(f\"Error: PPT file not found at {ppt_file_path}\")\n",
    "            return\n",
    "        \n",
    "        # Load the presentation\n",
    "        presentation = Presentation(ppt_file_path)\n",
    "        ppt_text = []\n",
    "\n",
    "        # Extract text from each slide\n",
    "        for slide in presentation.slides:\n",
    "            slide_text = []\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    slide_text.append(shape.text.strip())\n",
    "            ppt_text.append(\" \".join(slide_text))\n",
    "\n",
    "        # Combine all slide text\n",
    "        combined_text = \"\\n\\n\".join(ppt_text)\n",
    "\n",
    "        # Translate the extracted text, if needed\n",
    "        try:\n",
    "            translated_text = translate_hinglish(combined_text)\n",
    "        except Exception as translation_error:\n",
    "            translated_text = f\"Translation failed: {translation_error}\"\n",
    "\n",
    "        # Save the extracted and translated text to a file\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(\"Extracted Text from PPT:\\n\" + combined_text + \"\\n\\n\")\n",
    "            file.write(\"Translated Text (English):\\n\" + translated_text)\n",
    "\n",
    "        print(f\"Extracted text and translation saved to {output_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "ppt_file = \"phase 1 ppt.pptx\"\n",
    "if os.path.exists(ppt_file):\n",
    "    extract_text_from_ppt(ppt_file)\n",
    "else:\n",
    "    print(f\"PPT file '{ppt_file}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import textwrap  # Importing textwrap module\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('averaged_perceptron_tagger')  # Download POS Tagger\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def load_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read().lower()\n",
    "\n",
    "def extract_keywords(text, max_keywords=None):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Remove stop words and non-alphanumeric words\n",
    "    keywords = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Lemmatize each word to its base form\n",
    "    lemmatized_keywords = [lemmatizer.lemmatize(word) for word in keywords]\n",
    "\n",
    "    # Count word frequency\n",
    "    keyword_counts = Counter(lemmatized_keywords)\n",
    "    \n",
    "    # Sort keywords by frequency and limit them to max_keywords if specified\n",
    "    sorted_keywords = [keyword for keyword, _ in keyword_counts.most_common(max_keywords)]\n",
    "    \n",
    "    return set(sorted_keywords)\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())  # Add the synonym to the set\n",
    "    return synonyms\n",
    "\n",
    "def compare_keywords(keywords1, keywords2):\n",
    "    matches = set()\n",
    "    only_in_1 = set(keywords1)\n",
    "    only_in_2 = set(keywords2)\n",
    "    \n",
    "    # Find exact matches first\n",
    "    for word1 in keywords1:\n",
    "        if word1 in keywords2:\n",
    "            matches.add(word1)\n",
    "    \n",
    "    # Match words semantically using synonyms\n",
    "    for word1 in keywords1:\n",
    "        for word2 in keywords2:\n",
    "            if word1 != word2 and (word2 in get_synonyms(word1)):\n",
    "                matches.add(word1)\n",
    "\n",
    "    # Remove matched words from both sets\n",
    "    only_in_1 -= matches\n",
    "    only_in_2 -= matches\n",
    "    \n",
    "    accuracy = (len(matches) / len(keywords1)) * 100 if keywords1 else 0\n",
    "    return matches, only_in_1, only_in_2, accuracy\n",
    "\n",
    "def plot_graph(match_count, mismatch_count):\n",
    "    labels = ['Matching Keywords', 'Missing Keywords']\n",
    "    values = [match_count, mismatch_count]\n",
    "    colors = ['green', 'red']\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.bar(labels, values, color=colors)\n",
    "    plt.title(\"Keyword Match:presentation vs audio\")\n",
    "    plt.ylabel(\"Number of Keywords\")\n",
    "    plt.show()\n",
    "\n",
    "ppt_text = load_file(\"extracted_text.txt\")\n",
    "speech_text = load_file(\"transcription.txt\")\n",
    "max_keywords = 50\n",
    "ppt_keywords = extract_keywords(ppt_text, max_keywords=max_keywords)\n",
    "speech_keywords = extract_keywords(speech_text, max_keywords=max_keywords)\n",
    "\n",
    "def generate_feedback(matches, missing_in_speech, extra_in_speech, accuracy):\n",
    "    feedback = []\n",
    "\n",
    "    # Feedback on accuracy\n",
    "    if accuracy >= 90:\n",
    "        feedback.append(\"Excellent! Your speech closely matched the presentation content.\")\n",
    "    elif accuracy >= 75:\n",
    "        feedback.append(\"Good job! Most of the key points from the presentation were covered.\")\n",
    "    elif accuracy >= 50:\n",
    "        feedback.append(\"Fair attempt. You covered some key topics, but quite a few were missed.\")\n",
    "    else:\n",
    "        feedback.append(\"Needs Improvement. A large portion of the presentation content was not covered in the speech.\")\n",
    "\n",
    "    # Feedback on missing keywords\n",
    "    if missing_in_speech:\n",
    "        feedback.append(f\"You missed {len(missing_in_speech)} important keyword(s) from the presentation.\")\n",
    "        few_missed = list(sorted(missing_in_speech))[:10]\n",
    "        feedback.append(\"Missed keywords: \" + \", \".join(few_missed) + (\"...\" if len(missing_in_speech) > 10 else \"\"))\n",
    "\n",
    "    # Feedback on extra/unrelated keywords\n",
    "    if extra_in_speech:\n",
    "        feedback.append(f\"You included {len(extra_in_speech)} extra keyword(s) that were not part of the presentation.\")\n",
    "        few_extra = list(sorted(extra_in_speech))[:10]\n",
    "        feedback.append(\"Extra keywords: \" + \", \".join(few_extra) + (\"...\" if len(extra_in_speech) > 10 else \"\"))\n",
    "\n",
    "   \n",
    "    print(\"\\nFEEDBACK:\")\n",
    "    for line in feedback:\n",
    "        print(line)\n",
    "# Show feedback\n",
    "generate_feedback(matches, missing_in_speech, extra_in_speech, accuracy)\n",
    "\n",
    "# Compare\n",
    "matches, missing_in_speech, extra_in_speech, accuracy = compare_keywords(ppt_keywords, speech_keywords)\n",
    "\n",
    "# Output results with textwrap for better formatting\n",
    "print(\"\\n Keywords from PPT:\", len(ppt_keywords))\n",
    "print(\"Keywords from Transcription:\", len(speech_keywords))\n",
    "\n",
    "# Wrap the keywords and print them\n",
    "print(\"\\nKeywords from PPT:\")\n",
    "print(\"\\n\".join(textwrap.wrap(\", \".join(sorted(ppt_keywords)), width=80)))\n",
    "\n",
    "print(\"\\nKeywords from Transcription:\")\n",
    "print(\"\\n\".join(textwrap.wrap(\", \".join(sorted(speech_keywords)), width=80)))\n",
    "\n",
    "print(f\"\\n‚úÖ Matching Keywords ({len(matches)}):\")\n",
    "print(\"\\n\".join(textwrap.wrap(\", \".join(sorted(matches)), width=80)))\n",
    "\n",
    "print(f\"‚ùå Missing from Speech ({len(missing_in_speech)}):\")\n",
    "print(\"\\n\".join(textwrap.wrap(\", \".join(sorted(missing_in_speech)), width=80)))\n",
    "\n",
    "print(f\"‚ö†Ô∏è Extra in Speech ({len(extra_in_speech)}):\")\n",
    "print(\"\\n\".join(textwrap.wrap(\", \".join(sorted(extra_in_speech)), width=80)))\n",
    "\n",
    "print(f\"\\n Accuracy of Coverage: {accuracy:.2f}%\")\n",
    "\n",
    "# Show graph\n",
    "plot_graph(len(matches), len(missing_in_speech))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from pptx import Presentation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from fuzzywuzzy import fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# ‚úÖ Step 1: Extract full text from PowerPoint\n",
    "def extract_text_from_ppt(ppt_path):\n",
    "    prs = Presentation(ppt_path)\n",
    "    text = \"\"\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                text += shape.text + \" \"\n",
    "            if hasattr(shape, \"table\"):\n",
    "                for row in shape.table.rows:\n",
    "                    for cell in row.cells:\n",
    "                        text += cell.text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "# ‚úÖ Step 2: Preprocess text into keywords (clean, lemmatize, stem)\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text.lower())\n",
    "    keywords = []\n",
    "    for word in words:\n",
    "        if word not in stop_words and word not in string.punctuation and word.isalnum():\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            stem = stemmer.stem(lemma)\n",
    "            keywords.append(stem)\n",
    "    return set(keywords)\n",
    "\n",
    "# ‚úÖ Step 3: Fuzzy match keywords with score ‚â• 75\n",
    "def fuzzy_match(ppt_keywords, speech_keywords, threshold=75):\n",
    "    matched = set()\n",
    "    missed = set(ppt_keywords)\n",
    "    extra = set(speech_keywords)\n",
    "\n",
    "    for ppt_word in ppt_keywords:\n",
    "        for speech_word in speech_keywords:\n",
    "            score = fuzz.ratio(ppt_word, speech_word)\n",
    "            if score >= threshold:\n",
    "                matched.add(ppt_word)\n",
    "                missed.discard(ppt_word)\n",
    "                extra.discard(speech_word)\n",
    "                break\n",
    "\n",
    "    return matched, missed, extra\n",
    "\n",
    "# ‚úÖ Step 4: Accuracy calculation\n",
    "def calculate_accuracy(matched, total_keywords):\n",
    "    if len(total_keywords) == 0:\n",
    "        return 0\n",
    "    return (len(matched) / len(total_keywords)) * 100\n",
    "\n",
    "# ‚úÖ Step 5: Plot result\n",
    "def plot_results(matched, missed, extra):\n",
    "    categories = ['Matched', 'Missed', 'Extra']\n",
    "    values = [len(matched), len(missed), len(extra)]\n",
    "\n",
    "    colors = ['green', 'red', 'blue']\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.bar(categories, values, color=colors)\n",
    "    plt.title(\"PPT vs. Speech Keyword Comparison\")\n",
    "    plt.ylabel(\"Keyword Count\")\n",
    "\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 1, int(yval), ha='center', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ‚úÖ Step 6: Full run function\n",
    "def run_keyword_match(ppt_path, speech_path):\n",
    "    if not os.path.exists(ppt_path) or not os.path.exists(speech_path):\n",
    "        print(\"‚ùå PPT or transcription file not found.\")\n",
    "        return\n",
    "\n",
    "    ppt_text = extract_text_from_ppt(ppt_path)\n",
    "    with open(speech_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        speech_text = f.read()\n",
    "\n",
    "    ppt_keywords = preprocess_text(ppt_text)\n",
    "    speech_keywords = preprocess_text(speech_text)\n",
    "\n",
    "    matched, missed, extra = fuzzy_match(ppt_keywords, speech_keywords)\n",
    "\n",
    "    accuracy = calculate_accuracy(matched, ppt_keywords)\n",
    "    print(f\"\\n‚úÖ Total PPT Keywords: {len(ppt_keywords)}\")\n",
    "    print(f\"‚úÖ Matched Keywords: {len(matched)}\")\n",
    "    print(f\"‚ùå Missed from PPT: {len(missed)}\")\n",
    "    print(f\"üó£Ô∏è Extra from Speech: {len(extra)}\")\n",
    "    print(f\"üìà Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    plot_results(matched, missed, extra)\n",
    "\n",
    "# üìÅ File paths\n",
    "ppt_file = \"phase 1 ppt.pptx\"\n",
    "speech_file = \"transcription.txt\"\n",
    "\n",
    "run_keyword_match(ppt_file, speech_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
